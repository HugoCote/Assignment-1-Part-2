{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms # \n",
    "import torchvision.datasets.mnist as mnist  # to import data\n",
    "\n",
    "# we use torch.cuda.Event(enable_timing=True) to measure time\n",
    "# from timeit import default_timer as timer\n",
    "# import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import collections           # for ordered_dictionnary\n",
    "import torch.nn.init as init # to initialize model\n",
    "\n",
    "import copy                  # for copy.deepcopy( ... )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment 1, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "transform2 = transforms.Compose( [transforms.ToTensor(), transforms.Normalize( (0.1307,), (0.3081,)) ])\n",
    "dl_path  = \"./data_mnist\"\n",
    "\n",
    "trainset = mnist.MNIST( root = dl_path, train=True , download=True, transform = transform2 )\n",
    "testset  = mnist.MNIST( root = dl_path, train=False, download=True, transform = transform2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7     2     3     3     4     7     4     8\n",
      "img size =  torch.Size([8, 1, 28, 28]) label size =  torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "nb_sample = 8\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = nb_sample, shuffle=True , num_workers=2)\n",
    "testloader  = torch.utils.data.DataLoader(testset , batch_size = nb_sample, shuffle=False, num_workers=2)\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = ( img + 0.43 ) / 3.3    # to be in the interval [0,1]\n",
    "    npimg = img.numpy()\n",
    "    npimg = (255*npimg).astype(np.uint8) # to be a int in (0,...,255)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j].item() for j in range(nb_sample)))\n",
    "print( \"img size = \" , images.size() , \"label size = \" , labels.size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print size of train-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "training_dataset_size = trainset.__len__()\n",
    "testing_dataset_size  = testset.__len__()\n",
    "\n",
    "print( training_dataset_size )\n",
    "print( testing_dataset_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the device to cuda if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the MPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, h1, h2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, h1)\n",
    "        self.fc2 = nn.Linear(h1 , h2)\n",
    "        self.fc3 = nn.Linear(h2 , 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view( -1, 28 * 28 )\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "# print( nn.init._calculate_fan_in_and_fan_out(net.state_dict()[\"fc1.weight\"]) )\n",
    "# print( net.state_dict()[\"fc1.weight\"].size() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "architecture taken from :\n",
    "    https://github.com/MaximumEntropy/welcome_tutorials/tree/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Convnet Classifier\"\"\"\n",
    "    def __init__(self, kernel_sz = 3 ):\n",
    "        \n",
    "        if kernel_sz % 2 == 0 :\n",
    "            raise ValueError(\"kernel size must be odd\")\n",
    "        pad = kernel_sz // 2 \n",
    "        \n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=( kernel_sz , kernel_sz ), padding=pad ),\n",
    "            # nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=( kernel_sz , kernel_sz), padding=pad),\n",
    "            # nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=( kernel_sz , kernel_sz), padding=pad),\n",
    "            # nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(kernel_sz , kernel_sz), padding=pad),\n",
    "            # nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        )\n",
    "        # Logistic Regression\n",
    "        self.clf = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.clf(self.conv(x).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of parameters in each models and display the computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params =  877930  =   \n",
      " (fc1.weight)             620*784\n",
      " (fc1.bias)             + 620\n",
      " (fc2.weight)           + 620*620\n",
      " (fc2.bias)             + 620\n",
      " (fc3.weight)           + 10*620\n",
      " (fc3.bias)             + 10\n",
      "number of params =  873738  =   \n",
      " (conv.0.weight)          16*1*9*9\n",
      " (conv.0.bias)          + 16\n",
      " (conv.3.weight)        + 32*16*9*9\n",
      " (conv.3.bias)          + 32\n",
      " (conv.6.weight)        + 64*32*9*9\n",
      " (conv.6.bias)          + 64\n",
      " (conv.9.weight)        + 128*64*9*9\n",
      " (conv.9.bias)          + 128\n",
      " (clf.weight)           + 10*128\n",
      " (clf.bias)             + 10\n"
     ]
    }
   ],
   "source": [
    "def number_of_params( net ) :\n",
    "    nb_param  = 0\n",
    "    param_lst = \" \"\n",
    "    for i, (key, value) in enumerate( net.state_dict().items() ) :\n",
    "        \n",
    "        if i == 0 :\n",
    "            param_lst = param_lst + \"\\n ({:<20}    \".format( key + \")\" )\n",
    "        else :\n",
    "            param_lst = param_lst + \"\\n ({:<20}  + \".format( key + \")\" )\n",
    "            \n",
    "            \n",
    "        nb_param_tmp = 1\n",
    "        \n",
    "        for j , x in enumerate(value.size()) :\n",
    "            if j == 0 :\n",
    "                param_lst = param_lst + \"{xx}\".format( xx = x ) \n",
    "            else :\n",
    "                param_lst = param_lst + \"*{xx}\".format( xx = x ) \n",
    "                               \n",
    "            nb_param_tmp = nb_param_tmp * x\n",
    "                   \n",
    "        nb_param = nb_param + nb_param_tmp\n",
    "        \n",
    "    print( \"number of params = \" , nb_param , \" = \", param_lst  )\n",
    "\n",
    "mytestnet1 = MLP(h1=620,h2=620)\n",
    "mytestnet2 = Classifier( kernel_sz=9 )\n",
    "number_of_params( mytestnet1 )\n",
    "number_of_params( mytestnet2 )\n",
    "del mytestnet1\n",
    "del mytestnet2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some initialization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init ( layer ) :\n",
    "    \"\"\"\n",
    "    Weiths are generated from U[-d,d] where d = sqrt(6/(fan_in + fan_out)), biases are set to zero\n",
    "    \"\"\"\n",
    "    if type(layer) == nn.Linear or type(layer) == nn.Conv2d :\n",
    "        init.xavier_uniform_( layer.weight , gain=1 )\n",
    "        layer.bias.data.fill_(0.0)\n",
    "        \n",
    "def zero_init ( layer ) :\n",
    "    \"\"\"Everything is set to zero\"\"\"\n",
    "    if type(layer) == nn.Linear or type(layer) == nn.Conv2d :\n",
    "        layer.weight.data.fill_(0.0)\n",
    "        layer.bias.data.fill_(0.0)\n",
    "        \n",
    "def norm_init ( layer ) :\n",
    "    \"\"\"Weiths are generated from std normal, biases are set to zero\"\"\"\n",
    "    if type(layer) == nn.Linear or type(layer) == nn.Conv2d :\n",
    "        init.normal_(layer.weight, mean=0, std=1)\n",
    "        layer.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = MLP(500,500)\n",
    "# print( net.state_dict()[\"fc1.weight\"] )\n",
    "# print( net.state_dict()[\"fc1.bias\"] )\n",
    "# net.apply( one_step_Glorot_init )\n",
    "# print( net.state_dict()[\"fc1.weight\"])\n",
    "# print( net.state_dict()[\"fc1.bias\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    del cudanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(64, 128, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (clf): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cudanet = Classifier( kernel_sz = 9 )\n",
    "# cudanet = MLP(h1=620,h2=620)\n",
    "\n",
    "cudanet.apply( glorot_init )\n",
    "# cudanet.apply( zero_init )\n",
    "# cudanet.apply( norm_init )\n",
    "\n",
    "# cudanet.load_state_dict(torch.load(path), strict=False)\n",
    "cudanet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cudanet.parameters(), lr=0.055, momentum=0.0, weight_decay=0)\n",
    "nb_epoch  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss = 0.00347052\n",
      "epoch = 2, loss = 0.00084709\n",
      "epoch = 3, loss = 0.00039708\n",
      "epoch = 4, loss = 0.00031282\n",
      "epoch = 5, loss = 0.00021171\n",
      "epoch = 6, loss = 0.00018068\n",
      "epoch = 7, loss = 0.00015818\n",
      "epoch = 8, loss = 0.00014084\n",
      "epoch = 9, loss = 0.00012693\n",
      "epoch = 10, loss = 0.00011423\n",
      "Finished Training\n",
      "time required =  139.8591875  s \n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16*64,shuffle=True, num_workers=2)\n",
    "state_dict_list = list()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end   = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "\n",
    "for epoch in range( nb_epoch ):  # loop over the dataset multiple times\n",
    "    \n",
    "    # if epoch == 5 :\n",
    "    #   optimizer = optim.SGD(cudanet.parameters(), lr=0.05, momentum=0.0, weight_decay=0) \n",
    "        \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cudanet(inputs)\n",
    "        # if i == 0 : print( outputs )\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    else : # print every epoch\n",
    "        print('epoch = %d, loss = %.8f' % (epoch + 1, running_loss / (i*8*64))) # nb of sample per mini-batch\n",
    "        running_loss = 0.0\n",
    "        torch.cuda.synchronize()\n",
    "        tmp_state_dict = {}\n",
    "        for k, v in cudanet.state_dict().items():\n",
    "            tmp_state_dict[k] = v.cpu()\n",
    "        state_dict_list.append( tmp_state_dict )\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "else : \n",
    "    print('Finished Training')\n",
    "    \n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "print( \"time required = \" , start.elapsed_time(end)*0.001 , \" s \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test accuracy of net in its current state on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 91.18 %\n"
     ]
    }
   ],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=8*64,shuffle=True, num_workers=2)\n",
    "correct = torch.tensor([0])\n",
    "total = torch.tensor([0])\n",
    "\n",
    "correct, total = correct.to(device) , total.to(device) \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cudanet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # print(correct)\n",
    "        # print(predicted.size())\n",
    "        # print(labels.size())\n",
    "        # break\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the' , testing_dataset_size , 'test images: %.2f %%' \n",
    "          % ( (100 * correct.double()) / total.double()  )\n",
    "     ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the current state of the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False :\n",
    "    for key, value in cudanet.state_dict().items() :\n",
    "        pass\n",
    "        print( key , \" = \\n\" , value , end = \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 60000 train images: 98.32 %\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16*64,shuffle=True, num_workers=2)\n",
    "\n",
    "correct = torch.tensor([0])\n",
    "total = torch.tensor([0])\n",
    "\n",
    "correct, total = correct.to(device) , total.to(device) \n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cudanet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # print(correct)\n",
    "        # print(predicted)\n",
    "        # print(labels)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the' , training_dataset_size  , 'train images: %.2f %%' \n",
    "          % ( (100.0 * correct.double() ) / total.double() )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare accuracy across epoch on training-test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   1 : Accuracy of the network on the test images: 79.75 % , training images 79.11 % \n",
      "epoch   2 : Accuracy of the network on the test images: 92.44 % , training images 91.84 % \n",
      "epoch   3 : Accuracy of the network on the test images: 95.77 % , training images 95.63 % \n",
      "epoch   4 : Accuracy of the network on the test images: 96.42 % , training images 96.55 % \n",
      "epoch   5 : Accuracy of the network on the test images: 96.96 % , training images 97.02 % \n",
      "epoch   6 : Accuracy of the network on the test images: 97.54 % , training images 97.55 % \n",
      "epoch   7 : Accuracy of the network on the test images: 97.28 % , training images 97.31 % \n",
      "epoch   8 : Accuracy of the network on the test images: 97.76 % , training images 97.94 % \n",
      "epoch   9 : Accuracy of the network on the test images: 98.04 % , training images 98.19 % \n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 8*64, shuffle=True , num_workers=2)\n",
    "testloader  = torch.utils.data.DataLoader(testset , batch_size = 8*64, shuffle=True , num_workers=2)\n",
    "\n",
    "accuracy = torch.ones(nb_epoch,2, dtype=torch.float) * 100\n",
    "for epoch , tmp_state_dict in enumerate(state_dict_list,0) :\n",
    "    if epoch % 1 != 0 :\n",
    "        continue\n",
    "    \n",
    "    # net_model = cudanet.__class__\n",
    "    # cudaTOCPUnet = net_model(h1,h2)\n",
    "    # cudaTOCPUnet = Classifier( kernel_sz = 9 )\n",
    "    cudaTOCPUnet = copy.deepcopy( cudanet )\n",
    "    cudaTOCPUnet.load_state_dict( tmp_state_dict )\n",
    "    cuda_test_net = copy.deepcopy(cudaTOCPUnet).to(device)\n",
    "    \n",
    "    correct = torch.tensor([0,0])\n",
    "    total   = torch.tensor([0,0])\n",
    "    \n",
    "    correct, total = correct.to(device) , total.to(device) \n",
    "        \n",
    "    loader_list = [ testloader , trainloader ]\n",
    "    with torch.no_grad():\n",
    "        for i, loader in enumerate(loader_list,0) :\n",
    "            for data in loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cuda_test_net(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total[i] += labels.size(0)\n",
    "                correct[i] += (predicted == labels).sum()\n",
    "    \n",
    "    accuracy[epoch,:] = accuracy[epoch,:] * correct.type(torch.FloatTensor) / total.type(torch.FloatTensor)\n",
    "    \n",
    "    print('epoch %3d : Accuracy of the network on the test images: %.2f %% , training images %.2f %% ' \n",
    "              % ( epoch+1, accuracy[epoch,0] , accuracy[epoch,1] )\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the train and validation errors for each of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# data to plot\n",
    "n_groups = nb_epoch\n",
    "accuracy_toplot = accuracy.numpy()\n",
    "\n",
    "tests_accuracy = accuracy_toplot[:,0]\n",
    "train_accuracy = accuracy_toplot[:,1]\n",
    "\n",
    "#\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams[\"figure.figsize\"] = (12 ,8)\n",
    "# create plot\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(n_groups)\n",
    "bar_width = 0.3\n",
    "opacity = 0.8\n",
    " \n",
    "rects1 = plt.bar(index, tests_accuracy, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='g',\n",
    "                 label='Test set')\n",
    " \n",
    "rects2 = plt.bar(index + bar_width, train_accuracy, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 color='r',\n",
    "                 label='Training set')\n",
    "\n",
    "eps = 3 \n",
    "top = min(int( np.ceil(accuracy_toplot.max() + eps)) , 100)\n",
    "bot = max(int(np.floor(accuracy_toplot.min() - eps)) , 0  )\n",
    "\n",
    "\n",
    "plt.ylim(bot, top)     # set the ylim to bottom, top\n",
    "plt.axhline(y=97,color=\"black\")\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison between training set and test set accuracy \\nduring the training phase')\n",
    "plt.xticks(index + bar_width, range(1,11,1) )\n",
    "plt.yticks( range(bot,top+1,1) )\n",
    "plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some additionnal stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3-best accuracy : having the right label in the net's top 3 answers count as a good answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-best Accuracy of the network on the  10000  test images: 99.920 %\n"
     ]
    }
   ],
   "source": [
    "correct = torch.tensor([0])\n",
    "total = torch.tensor([0])\n",
    "\n",
    "correct, total = correct.to(device) , total.to(device) \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cudanet(images)\n",
    "        predicted = torch.topk(outputs.data, 3)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted[:,0] == labels).sum()\n",
    "        correct += (predicted[:,1] == labels).sum()\n",
    "        correct += (predicted[:,2] == labels).sum()\n",
    "\n",
    "print('3-best Accuracy of the network on the ' , testing_dataset_size , ' test images: %.3f %%' \n",
    "          % ( 100 * correct.double() / total.double())\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print some test sample that the net misclassifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this should be :   8, 4, 6, 9, 5, 6\n",
      "net choice # 1 :   3, 2, 0, 7, 3, 0\n",
      "net choice # 2 :   8, 4, 6, 9, 5, 6\n",
      "net choice # 3 :   2, 6, 5, 8, 8, 5\n"
     ]
    }
   ],
   "source": [
    "nb_of_error = torch.tensor([6])\n",
    "j = torch.tensor([0])\n",
    "errorimages  = torch.empty(6,1,28,28)\n",
    "errorlabels  = torch.empty(6)\n",
    "erroroutputs = torch.empty(6, 10)\n",
    "for images, labels in testloader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = cudanet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    if not (predicted == labels).sum() == labels.size()[0] :\n",
    "        comparison = (predicted == labels)\n",
    "        for i,b in enumerate( comparison ) :\n",
    "            if b == 0 :\n",
    "                errorimages[j,:,:,:] = copy.deepcopy(images[i,:,:,:]).cpu()\n",
    "                errorlabels[j] = labels[i].clone().detach().requires_grad_(False).cpu()\n",
    "                erroroutputs[j,:] = outputs[i,:].clone().detach().requires_grad_(False).cpu()\n",
    "                j = j + 1\n",
    "                if j.item() >= nb_of_error.item() :\n",
    "                    break\n",
    "        else :\n",
    "            continue\n",
    "        break\n",
    "        \n",
    "if j.item() == 0 :\n",
    "    print( \"no error found\")\n",
    "else :\n",
    "    imshow( torchvision.utils.make_grid(errorimages) )\n",
    "    \n",
    "    print( \"this should be : \" , \",\".join( \"%2d\" % nb.item() for nb in errorlabels ) ) \n",
    "    for i in range(3) :\n",
    "        print( \"net choice #\" , i+1 , \": \" , \",\".join( \"%2d\" % nb.item() for nb in list(torch.topk( erroroutputs, 3)[1][:,i]) ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the state_dict of the model for each epoch on a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./save/dev1num2model_for_epoch1.pth\n",
      "./save/dev1num2model_for_epoch2.pth\n",
      "./save/dev1num2model_for_epoch3.pth\n",
      "./save/dev1num2model_for_epoch4.pth\n",
      "./save/dev1num2model_for_epoch5.pth\n",
      "./save/dev1num2model_for_epoch6.pth\n",
      "./save/dev1num2model_for_epoch7.pth\n",
      "./save/dev1num2model_for_epoch8.pth\n",
      "./save/dev1num2model_for_epoch9.pth\n",
      "./save/dev1num2model_for_epoch10.pth\n"
     ]
    }
   ],
   "source": [
    "local_path = \"./save\"\n",
    "for epoch , tmp_state_dict in enumerate(state_dict_list,1) :\n",
    "    saving_path = local_path + \"/dev1num2model_for_epoch{Epoch}.pth\".format( Epoch = epoch )\n",
    "    torch.save( tmp_state_dict , saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load from file and set the load the state_dict of the last epoch on a object\n",
    "the files have to be located in \"./save\" and named \"dev1num2model_for_epoch{j}.pth\" for j from ... to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./save/dev1num2model_for_epoch1.pth\n",
      "./save/dev1num2model_for_epoch2.pth\n",
      "./save/dev1num2model_for_epoch3.pth\n",
      "./save/dev1num2model_for_epoch4.pth\n",
      "./save/dev1num2model_for_epoch5.pth\n",
      "./save/dev1num2model_for_epoch6.pth\n",
      "./save/dev1num2model_for_epoch7.pth\n",
      "./save/dev1num2model_for_epoch8.pth\n",
      "./save/dev1num2model_for_epoch9.pth\n",
      "./save/dev1num2model_for_epoch10.pth\n"
     ]
    }
   ],
   "source": [
    "local_path = \"./save\"\n",
    "state_dict_list = list()\n",
    "\n",
    "from_idx = 1\n",
    "to_idx = 10\n",
    "\n",
    "for epoch in range( from_idx  , to_idx + 1 , 1 ):\n",
    "    path = local_path + \"/dev1num2model_for_epoch{Epoch}.pth\".format( Epoch = epoch )\n",
    "    tmp_dict = torch.load(path)\n",
    "    state_dict_list.append(tmp_dict)\n",
    "    print( path )\n",
    "else : \n",
    "    cudanet = Classifier( kernel_sz = 9 )\n",
    "    cudanet.load_state_dict(tmp_dict)\n",
    "    cudanet.eval()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
